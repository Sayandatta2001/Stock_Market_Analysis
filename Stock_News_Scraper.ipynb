{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUFkGQ0AIJiD"
      },
      "outputs": [],
      "source": [
        "class console_colors:  # for printing color texts to console\n",
        "    HIGHLIGHT = \"\\x1b[6;30;42m\"\n",
        "    HIGHLIGHT_END = \"\\x1b[0m\"\n",
        "    HEADER = \"\\033[95m\"\n",
        "    OKBLUE = \"\\033[94m\"\n",
        "    OKCYAN = \"\\033[96m\"\n",
        "    OKGREEN = \"\\033[92m\"\n",
        "    WARNING = \"\\033[93m\"\n",
        "    FAIL = \"\\033[91m\"\n",
        "    ENDC = \"\\033[0m\"\n",
        "    BOLD = \"\\033[1m\"\n",
        "    UNDERLINE = \"\\033[4m\"\n",
        "\n",
        "\n",
        "import sys\n",
        "import json\n",
        "import signal\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import quote_plus as url_encode\n",
        "from datetime import datetime, timedelta\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# * Inputs\n",
        "# ----------------------------\n",
        "query = \"Tata Motors\"\n",
        "starting_date = \"11/16/2020\"  # ? MM/DD/YYYY without leading zeroes\n",
        "ending_date = \"6/28/2023\"  # ? MM/DD/YYYY without leading zeroes\n",
        "max_req_limit = 100  #! your IP can get banned if you give to much requests (maybe 2500 req/ day, but being on the safe side by setting it to 100)\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:77.0) Gecko/20100101 Firefox/77.0\"\n",
        "}\n"
      ],
      "metadata": {
        "id": "Dur_2fzFIlrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "starting_datetime = datetime.strptime(starting_date, \"%m/%d/%Y\")\n",
        "ending_datetime = datetime.strptime(ending_date, \"%m/%d/%Y\")\n",
        "current_datetime = starting_datetime\n",
        "one_day = timedelta(days=1)\n",
        "\n",
        "num_req = 0\n",
        "\n",
        "news = (\n",
        "    []\n",
        ")  # ? list of object {\"title\":'...', \"link\":'...', \"upload_time\":'...'} (upload_time is like '26 min ago\")\n",
        "URL_safe_query = url_encode(query)\n",
        "\n",
        "# google presents search in many pages (with maybe 10 queries in each page\n",
        "num_page = 1\n",
        "\n",
        "# following error message can be changed in future\n",
        "error_message = \"did not match any news results.\"\n",
        "\n",
        "# If you are not from india, you need to change the URL, using following steps\n",
        "# - Go To Google and search anything\n",
        "# - Go To News Tab\n",
        "# - select tools->select date->custom range and select any range\n",
        "# - Scroll to bottom of page and select 2 , ie the second page of google result\n",
        "# - fill q={URL_safe_query},  cd_min={starting_date} and cd_max={ending_date}\n",
        "\n",
        "\n",
        "def save_news_JSON(file_name):\n",
        "    print(\n",
        "        f\"{console_colors.OKCYAN}Saving Data in /news_data/{file_name}.{console_colors.ENDC}\"\n",
        "    )\n",
        "    with open(\"news_data/\" + file_name, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(news, f, ensure_ascii=False, indent=4)\n",
        "\n"
      ],
      "metadata": {
        "id": "YTva4794JZe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def signal_handler(sig, frame):\n",
        "    save_news_JSON(\"fallback.json\")\n",
        "    print(f\"{console_colors.FAIL}You pressed Ctrl+C!{console_colors.ENDC}\")\n",
        "    print(\"Data until now is saved as fallback.json\")\n",
        "    sys.exit(0)\n",
        "\n",
        "\n",
        "signal.signal(signal.SIGINT, signal_handler)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gB_o19E3Jmae",
        "outputId": "c737c3e0-399f-4ac1-bcec-5d5fa0447145"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function _signal.default_int_handler(signalnum, frame, /)>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_news():\n",
        "    global num_req, num_page, current_datetime, URL_safe_query, news\n",
        "\n",
        "    num_page = 1\n",
        "    news = []\n",
        "\n",
        "    while True:\n",
        "        if num_req >= max_req_limit:\n",
        "            print(\n",
        "                f\"{console_colors.FAIL}Reached Max Request Limit: {console_colors.ENDC}\",\n",
        "                max_req_limit,\n",
        "            )\n",
        "            sys.exit()\n",
        "            break\n",
        "\n",
        "        url = f\"https://www.google.com/search?q={URL_safe_query}&tbs=cdr:1,cd_min:{current_datetime.strftime('%m/%d/%Y')},cd_max:{(current_datetime).strftime('%m/%d/%Y')}&tbm=nws&start={((num_page -1) * 10)}\"\n",
        "\n",
        "        page = requests.get(url, headers=headers)\n",
        "        num_req += 1\n",
        "\n",
        "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "        if error_message in soup.body.text:\n",
        "            print(f\"{console_colors.OKGREEN}All News Scraped.{console_colors.ENDC}\")\n",
        "            break\n",
        "\n",
        "        print(f\"  - getting page number {num_page} of google search:\")\n",
        "        print(f\"    URL: {url}\")\n",
        "\n",
        "        titles = soup.select(\n",
        "            \"div.n0jPhd.ynAwRc.MBeuO.nDgy9d\"\n",
        "        )  # ? selector can change in future.It it does not work, Use developer tools of your browser to inspect and find a working selecto\n",
        "        links = soup.select(\n",
        "            \"a.WlydOe\"\n",
        "        )  # ? selector can change in future.It it does not work, Use developer tools of your browser to inspect and find a working selector\n",
        "        upload_time = soup.select(\".OSrXXb.rbYSKb.LfVVr\")\n",
        "\n",
        "        assert len(titles) == len(\n",
        "            links\n",
        "        ), \"The CSS Selectors are Outdated as number of links , titles and upload times are not equal. Edit the selectors or change a code near this assert statement to save this data\"\n",
        "\n",
        "        for i in range(len(titles)):\n",
        "            news.append(\n",
        "                {\n",
        "                    \"title\": titles[i].text,\n",
        "                    \"link\": links[i][\"href\"],\n",
        "                    \"upload_time\": upload_time[i].text,\n",
        "                }\n",
        "            )\n",
        "\n",
        "        num_page += 1\n",
        "    # ----------LOOP ENDED-------------------------------------\n",
        "    save_news_JSON(f\"news_data_{current_datetime.strftime('%d-%m-%Y')}.json\")\n",
        "\n"
      ],
      "metadata": {
        "id": "4q6SYptNJp5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "while True:\n",
        "    if current_datetime > ending_datetime:\n",
        "        print(f\"{console_colors.OKGREEN}All Data Scraped.{console_colors.ENDC}\")\n",
        "        break\n",
        "    print(\n",
        "        f\"{console_colors.HIGHLIGHT}getting news on given query on date : {console_colors.HIGHLIGHT_END}\",\n",
        "        current_datetime.strftime(\"%d-%m-%Y\"),\n",
        "    )\n",
        "    get_news()\n",
        "    print(\"--------------------------------------------------------------\")\n",
        "    current_datetime = current_datetime + one_day"
      ],
      "metadata": {
        "id": "W4n6wOg2JsF6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}